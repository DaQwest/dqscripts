
<h2>Christofides algorithm</h2>
<p>

(1) A heuristic algorithm to find a near-optimal solution to the traveling salesman problem.  Step 1: find a minimum spanning tree T.  Step 2: find a perfect matching M among vertices with odd degree.  Step 3: combine the edges of M and T to make a multigraph G.  Step 4: find an Euler cycle in G by skipping vertices already seen.  (2) An algorithm to find the chromatic number of a graph.
</p>
<h2>Beam stack search</h2>
<h2>Constraint satisfaction</h2>
<h2>AC-3 algorithm</h2>
<h2>Difference map algorithm</h2>
<h2>DPLL algorithm</h2>
<h2>Exact cover</h2>
<h2>Algorithm X</h2>
<h2>Dancing Links</h2>
<h2>Cross-entropy method</h2>
<h2>Differential evolution</h2>
<h2>Dynamic Programming</h2>
<h2>Evolutionary computation</h2>
<h2>Evolution strategy</h2>
<h2>Gene expression programming</h2>
<h2>Genetic algorithms</h2>
<h2>Truncation selection</h2>
<h2>Memetic algorithm</h2>
<h2>Swarm intelligence</h2>
<h2>Ant colony optimization</h2>
<h2>Bees algorithm</h2>
<h2>Particle swarm optimization</h2>
<h2>Gradient descent</h2>
<h2>Interior point method</h2>
<h2>Linear programming</h2>
<h2>Benson's algorithm</h2>
<h2>Dantzigâ€“Wolfe decomposition</h2>
<h2>Delayed column generation</h2>
<h2>Integer linear programming</h2>
<h2>Branch and cut</h2>
<h2>Cutting-plane method</h2>
<h2>Karmarkar's algorithm</h2>
<h2>Simplex algorithm</h2>
<h2>Line search</h2>
<h2>Local search (optimization)</h2>
<h2>Random-restart hill climbing</h2>
<h2>Tabu search</h2>
<h2>Minimax</h2>
<h2>Nearest neighbor search</h2>
<h2>Best Bin First</h2>
<h2>Newton's method in optimization</h2>
<h2>Nonlinear optimization</h2>
<h2>BFGS method</h2>
<h2>Gaussâ€“Newton algorithm</h2>
<h2>Levenbergâ€“Marquardt algorithm</h2>
<h2>Nelderâ€“Mead method</h2>
<h2>Odds algorithm</h2>
<h2>Simulated annealing</h2>
<h2>Stochastic tunneling</h2>
<h2>Subset sum problem</h2>
<h2>Pagoda (data structure)</h2>
<p>

A priority queue implemented with a variant of a binary tree.  The root points to its children, as in a binary tree.  Every other node points back to its parent and down to its leftmost (if it is a right child) or rightmost (if it is a left child) descendant leaf.  The basic operation is merge or meld, which maintains  the heap property.  An element is inserted by merging it as a singleton.  The root is removed by merging its right and left children.  Merging is bottom-up, merging the leftmost edge of one with the rightmost edge of the other.
</p>
<h2>Splay tree</h2>
<p>

A binary search tree in which operations that access nodes restructure the tree.
</p>
<p>
A self-organizing data structure which uses rotations to move any accessed key to the root. This leaves recently accessed nodes near the top of the tree, making them very quickly searchable (Skiena 1997, p.Â 177).
</p>
<h2>Treap</h2>
<p>

A binary search tree in which nodes have a randomly assigned priority.  Updates keep priorities in heap order instead of keeping balance information and doing rebalance operations.
</p>
<h2>AVL tree</h2>
<p>

A balanced binary search tree where the height of the two subtrees (children) of a node differs by at most one. Look-up, insertion, and deletion are O(log n), where n is the number of nodes in the tree.
</p>
<h2>Binary search tree</h2>
<p>

A binary tree where every node's left  subtree has keys less than the node's key, and every right subtree has keys greater than the node's key.
</p>
<h2>2-3 tree</h2>
<p>

A B-tree of order 3, that is, internal nodes have two or three children.
</p>
<p>
-trees were introduced by Bayer (1972) and McCreight. They are
 a special -ary balanced tree used in databases because their
 structure allows records to be inserted, deleted, and retrieved with guaranteed worst-case
 performance. An -node -tree has height
 , where lg is the logarithm to base 2. The AppleÂ® MacintoshÂ® (Apple,
 Inc., Cupertino, CA) HFS filing system uses -trees to store
 disk directories (Benedict 1995). A -tree satisfies
 the following properties: 
</p>
<h2>Binary heap</h2>
<p>
A sequence  forms
 a (binary) heap if it satisfies  for , where
  is the floor function,
 which is equivalent to  and 
 for . The first member must therefore
 be the smallest. A heap can be viewed as a labeled binary
 tree in which the label of the th node is smaller
 than the labels of any of its descendents (Skiena 1990, p.Â 35). Heaps support
 arbitrary insertion and seeking/deletion of the minimum value in  times per
 update (Skiena 1990, p.Â 38).
</p>
<p>

A complete binary tree where every node has a key more extreme (greater or less) than or equal to the key of its  parent.
</p>
<h2>Binomial heap</h2>
<p>

A heap made of a forest of binomial trees with the heap property numbered k=0, 1, 2, ..., n, each containing either 0 or 2k nodes. Each tree is formed by linking two of its predecessors, by joining one at the root of the other.  The operations of insert a value, decrease a value, delete a value, and merge or join (meld) two queues take O(log n) time.  The find minimum operation is a constant Î˜(1).
</p>
<h2>Ternary tree</h2>
<h2>Spaghetti stack</h2>
<h2>Exponential tree</h2>
<h2>Implicit kd-tree</h2>
<h2>Z-order (curve)</h2>
<h2>BK-tree</h2>
<h2>Data clustering</h2>
<h2>Canopy clustering algorithm</h2>
<h2>Complete-linkage clustering</h2>
<h2>Expectation-maximization algorithm</h2>
<h2>Fuzzy clustering</h2>
<h2>FLAME clustering</h2>
<h2>K-means clustering</h2>
<h2>K-means++</h2>
<h2>K-medoids</h2>
<h2>Lloyd's algorithm</h2>
<h2>Single-linkage clustering</h2>
<h2>SUBCLU</h2>
<h2>Ward's method</h2>
<h2>Estimation theory</h2>
<h2>Ordered subset expectation maximization</h2>
<h2>Odds algorithm</h2>
<h2>Kalman filter</h2>
<h2>FNN algorithm</h2>
<h2>Hidden Markov model</h2>
<h2>Baum–Welch algorithm</h2>
<h2>Forward-backward algorithm</h2>
<h2>Viterbi algorithm</h2>
<h2>Partial least squares regression</h2>
<h2>Queuing theory</h2>
<h2>Buzen's algorithm</h2>
<h2>Scoring algorithm</h2>
<h2>Yamartino method</h2>
<h2>Ziggurat algorithm</h2>
<h2>ALOPEX</h2>
<h2>Association rule learning</h2>
<h2>Apriori algorithm</h2>
<h2>One-attribute rule</h2>
<h2>Boosting (meta-algorithm)</h2>
<h2>BrownBoost</h2>
<h2>LogitBoost</h2>
<h2>Bootstrap aggregating</h2>
<h2>Decision tree learning</h2>
<h2>C4.5 algorithm</h2>
<h2>ID3 algorithm</h2>
<h2>K-nearest neighbors</h2>
<h2>Linde–Buzo–Gray algorithm</h2>
<h2>Locality-sensitive hashing</h2>
<h2>Artificial neural network</h2>
<h2>Backpropagation</h2>
<h2>Hopfield net</h2>
<h2>Pulse-coupled neural networks</h2>
<h2>Radial basis function network</h2>
<h2>Self-organizing map</h2>
<h2>Random forest</h2>
<h2>Q-learning</h2>
<h2>Relevance Vector Machine</h2>
<h2>Winnow algorithm</h2>
<h2>Bubble sort</h2>
<p>

Sort by comparing each adjacent pair of items in a list in turn, swapping the items if necessary, and repeating the pass through the list until no swaps are done.
</p>
<h2>Cocktail sort</h2>
<p>

A variant of bubble sort that compares each adjacent pair of items in a list in turn, swapping them if necessary, and alternately passes through the list from the beginning to the end then from the end to the beginning. It stops when a pass does no swaps.
</p>
<h2>Comb sort</h2>
<p>

An in-place sort algorithm that repeatedly reorders different pairs of items.  On each pass swap pairs of items separated by the increment or gap, if needed, and reduce the gap (divide it by about 1.3).  The gap starts at about 3/4 of the number of items.  Continue until the gap is 1 and a pass had no swaps.
</p>
<h2>Gnome sort</h2>
<p>

Put items in order by comparing the current item with the previous item.  If they are in order, move to the next item (or stop if the end is reached).  If they are out of order, swap them and move to the previous item.  If there is no previous item, move to the next item.
</p>
<h2>Odd-even sort</h2>
<h2>Quicksort</h2>
<p>

Pick an element from the array (the pivot), partition the remaining elements into those greater than and less than this pivot, and recursively sort the partitions. There are many variants of the basic scheme above: to select the pivot, to partition the array, to stop the recursion on small partitions, etc.
</p>
<p>
Quicksort is the fastest known comparison-based sorting algorithm (on average, and for a large number of elements),
 requiring  steps. Quicksort is a recursive
 algorithm which first partitions an array  according
 to several rules (Sedgewick 1978): 
</p>
<h2>Bogosort</h2>
<p>

A terribly inefficient sort algorithm that repeatedly generates a random permutation of the items until the items are in order.
</p>
<h2>Stooge sort</h2>
<p>

A terribly inefficient sort algorithm that swaps the top and bottom items if needed, then (recursively) sorts the bottom two-thirds, then the top two-thirds, then the bottom two-thirds again.
</p>
<h2>Flashsort</h2>
<p>

(no definition here, yet, but
you can help.)
</p>
<h2>Introsort</h2>
<p>

A variant of quicksort which switches to heapsort for pathological inputs, that is, when execution time is becoming quadratic.
</p>
<h2>Timsort</h2>
<h2>Insertion sort</h2>
<p>

Sort by repeatedly taking the next item and inserting it into the final data structure in its proper order with respect to items already inserted.  Run time is O(n2) because of moves.
</p>
<h2>Library sort</h2>
<h2>Patience sorting</h2>
<h2>Shell sort</h2>
<p>

The first diminishing increment sort.  On each pass i sets of n/i items are sorted, typically with insertion sort.  On each succeeding pass, i is reduced until it is 1 for the last pass.  A good series of i values is important to efficiency.
</p>
<p>
A sorting method proposed by Shell (1959) in which records being sorted can take long jumps instead of being restricted to short steps.

Knuth, D. E. The Art of Computer Programming, Vol. 3: Sorting and Searching, 2nd ed.
 Reading, MA: Addison-Wesley, pp. 83-95, 1998.
</p>
<h2>Tree sort</h2>
<h2>Strand sort</h2>
<p>

A sort algorithm that works well if many items are in order. First, begin a sublist by moving the first item from the original list to the sublist.  For each subsequent item in the original list, if it is greater than the last item of the sublist, remove it from the original list and append it to the sublist. Merge the sublist into a final, sorted list.  Repeatedly extract and merge  sublists until all items are sorted.  Handle two or fewer items as special cases.
</p>
<h2>Bead sort</h2>
<p>Embed Interactive Demonstration New!Files require Wolfram CDF Player or Mathematica.</p>
<h2>Burstsort</h2>
<h2>Counting sort</h2>
<p>

A 2-pass sort algorithm that is efficient when the range of keys is small and there many duplicate keys.  The first pass counts the occurrences of each key in an auxiliary array, and then makes a running total so each auxiliary entry is the number of preceding keys.  The second pass puts each item in its final place according to the auxiliary entry for that key.
</p>
<h2>Pigeonhole sort</h2>
<p>

A 2-pass sort algorithm that is efficient when the range of keys is approximately equal to the number of items.  The first pass allocates an array of buckets, one bucket for each possible key value, then moves each item to its key's bucket.  The second pass goes over the bucket array moving each item to the next place in the destination.
</p>
<h2>Postman sort</h2>
<p>

A highly engineered variant of top-down radix sort where attributes of the key are described so the algorithm can allocate buckets and distribute efficiently.
</p>
<h2>Radix sort</h2>
<p>

A multiple pass distribution sort algorithm that distributes each item to a bucket according to part of the item's key beginning with the least significant part of the key. After each pass, items are collected from the buckets, keeping the items in order, then redistributed according to the next most significant part of the key.
</p>
<p>
National Institute of Standards and Technology. "Radix Sort." http://www.nist.gov/dads/HTML/radixsort.html.
The #1 tool for creating Demonstrations and anything technical.Explore anything with the first computational knowledge engine.</p>
<h2>Heapsort</h2>
<p>

A sort algorithm that builds a heap, then repeatedly extracts the maximum item.  Run time is  O(n log n).
</p>
<p>
An sorting algorithm
 which is not quite as fast as quicksort. It is a "sort-in-place"
 algorithm and requires no auxiliary storage, which makes it particularly concise
 and elegant to implement.

Knuth, D. E. The Art of Computer Programming, Vol. 3: Sorting and Searching, 2nd ed.
 Reading, MA: Addison-Wesley, pp. 144-148, 1998.
</p>
<h2>Selection sort</h2>
<p>
A sorting algorithm which makes  passes over a set
 of  elements, in each pass selecting the smallest element
 and deleting it from the set. This algorithm has running time , compared
 to  for the best algorithms (Skiena 1990, p. 14).
</p>
<p>

A sort algorithm that repeatedly looks through remaining items to find the least one and moves it to its final location.  The run time is Θ(n²), where n is the number of elements.  The number of swaps is  O(n).
</p>
<h2>Smoothsort</h2>
<p>

A variant of heapsort that takes advantage of a partially ordered table.  Performance is O(n) when input is sorted and O(n log n)  performance for worst case.
</p>
<h2>Bitonic sorter</h2>
<p>

Compare, and swap if necessary, pairs of elements in parallel. Subsets are sorted then merged.
</p>
<h2>Pancake sorting</h2>
<p>
Assume that  numbered pancakes are stacked, and that
 a spatula can be used to reverse the order of the top  pancakes for . Then the pancake sorting problem asks
 how many such "prefix reversals" are sufficient to sort an arbitrary stack
 (Skiena 1990, p. 48).
</p>
<h2>Topological sort</h2>
<p>

To arrange items when some pairs of items have no comparison, that is, according to a partial order.
</p>
<p>
A topological sort is a permutation  of the vertices
 of a graph such that an edge  implies that
  appears before  in  (Skiena 1990, p. 208).
 Only directed acyclic graphs can be topologically
 sorted. The topological sort of a graph can be computed using TopologicalSort[g]
 in the Mathematica
 package Combinatorica` .
</p>
<h2>Samplesort</h2>
<h2>Coloring algorithm</h2>
<p>
A vertex coloring is an assignment of labels or colors to each vertex of a graph such that no edge connects two identically colored vertices. The most common type
 of vertex coloring seeks to minimize the number of colors for a given graph. Such
 a coloring is known as a minimum vertex coloring,
 and the minimum number of colors which with the vertices of a graph  may be colored
 is called the chromatic number, denoted .
</p>
<h2>Hopcroft–Karp algorithm</h2>
<h2>Prufer sequence</h2>
<p>
An encoding which provides a bijection between the labeled
 trees on  nodes and strings of  integers chosen
 from an alphabet of the numbers 1 to . A labeled
 tree can be converted to a Prüfer code using LabeledTreeToCode[g]
 in the Mathematica
 package Combinatorica` , and a code can be converted to a labeled
 tree using CodeToLabeledTree[code].
</p>
<h2>Tarjan's off-line least common ancestors algorithm</h2>
<h2>Force-based algorithms (graph drawing)</h2>
<p>Mathematica provides functions for the aesthetic drawing of graphs. Algorithms implemented include spring embedding, spring-electrical embedding, high-dimensional embedding, radial drawing, random embedding, circular embedding, and spiral embedding. In addition, algorithms for layered/hierarchical drawing of directed graphs as well as for the drawing of trees are available. These algorithms are implemented via four functions: GraphPlot, GraphPlot3D, LayeredGraphPlot, and TreePlot.</p>
<h2>Spectral layout</h2>
<h2>Girvan–Newman algorithm</h2>
<h2>Hyperlink-Induced Topic Search</h2>
<p>HITSCentrality[g] gives a list of authority and hub centralities for the vertices in the graph g.Compute HITS hub and authority centralities:Chart of the data:Rate web pages using hyperlink-induced topic search:</p>
<h2>Hubs and authorities</h2>
<h2>PageRank algorithm</h2>
<h2>TrustRank algorithm</h2>
<h2>Dinic's algorithm</h2>
<h2>Edmonds–Karp algorithm</h2>
<h2>Ford–Fulkerson algorithm</h2>
<p>

Given a flow function and its corresponding residual graph (a maximum-flow problem), select a path from the source to the sink along which the flow can be increased and increase the flow. Repeat until there are no such paths.
</p>
<h2>Karger's algorithm</h2>
<h2>Push–relabel algorithm</h2>
<h2>Euclidean minimum spanning tree</h2>
<h2>Longest path problem</h2>
<p>
The longest path problem asks to find a path of maximum length in a given graph. The problem is NP-complete, but there exists
 an efficient dynamic programming solution
 for acyclic digraphs.

The detour matrix is a real symmetric matrix whose th entry is the length of the longest
 path from vertex  to vertex .
</p>
<h2>Prim's algorithm</h2>
<h2>Reverse-delete algorithm</h2>
<h2>Nonblocking Minimal Spanning Switch</h2>
<h2>Dijkstra's algorithm</h2>
<p>
An algorithm for finding a graph geodesic, i.e., the shortest path between two graph
 vertices in a graph. It functions by constructing a
 shortest-path tree from the initial vertex to every other vertex in the graph. The
 algorithm is implemented as Dijkstra[g]
 in the Mathematica
 package Combinatorica` .
</p>
<p>

An algorithm to find the shortest paths from a single source vertex to all other vertices in a weighted, directed graph.  All weights must be nonnegative.
</p>
<h2>Johnson algorithm</h2>
<p>

An algorithm to solve the all pairs shortest path problem in a sparse weighted, directed graph.  First, it adds a new node with zero weight edges from it to all other nodes, and runs the Bellman-Ford algorithm to check for negative weight cycles and find h(v), the least weight of a path from the new node to node v.  Next it reweights the edges using the nodes' h(v) values.  Finally for each node, it runs Dijkstra's algorithm and stores the computed least weight to other nodes, reweighted using the nodes' h(v) values, as the final weight. The time complexity is O(V²log V + VE).
</p>
<h2>Transitive closure</h2>
<p>
The transitive closure of a binary relation  on a set  is the minimal
 transitive relation  on  that contains
 . Thus  for any elements
  and  of  provided that
 there exist , , ...,  with , , and  for all .
</p>
<p>

An extension or superset of a binary relation such that whenever (a,b) and (b,c) are in the extension, (a,c) is also in the extension.
</p>
<h2>Nearest neighbour algorithm</h2>
<p>
The problem in computational geometry of identifying the point from a set of points which is nearest to a given point according
 to some measure of distance. The nearest neighborhood problem involves identifying
 the locus of points lying nearer to the query point than to any other point in the
 set.
</p>
<h2>Knight's Tour</h2>
<p>To see full output on this page you need to enable JavaScript in your browser. More info »Zoom in to see an enlarged view of any output.AdvertisementBring Wolfram|Alpha output to life with CDF interactivity.</p>
<h2>A star algorithm</h2>
<h2>B star algorithm</h2>
<h2>Beam search</h2>
<h2>Best-first search</h2>
<p>

A state-space search algorithm that considers the estimated best partial solution next.  This is typically implemented with a priority queue.
</p>
<h2>Bidirectional search</h2>
<h2>Bloom filter</h2>
<p>

A probabilistic algorithm to quickly test membership in a large set using multiple hash functions into a single  array of bits.
</p>
<h2>D star algorithm</h2>
<h2>Incremental heuristic search</h2>
<h2>Iterative deepening depth-first search</h2>
<h2>Lexicographic breadth-first search</h2>
<h2>Uniform-cost search</h2>
<h2>Tree traversal</h2>
<h2>SSS star algorithm</h2>
<h2>Bron–Kerbosch algorithm</h2>
<p>
The Bron-Kerbosch algorithm is an efficient method for finding all maximal
cliques in a graph.

Akkoyunlu, E. A. "The Enumeration of Maximal Cliques of Large Graphs."
SIAM J. Comput. 2, 1-6, 1973.

Bron, C. and Kerbosch, J. "Algorithm 457: Finding All Cliques of an Undirected
Graph." Comm. ACM 16, 48-50, 1973.
</p>
<h2>Path-based strong component algorithm</h2>
<h2>Kosaraju's algorithm</h2>
<h2>Dynamic programming</h2>
<p>

Solve an optimization problem by caching subproblem solutions (memoization) rather than recomputing them.
</p>
<h2>Approximate string matching</h2>
<p>

Searching for approximate (e.g., up to a predefined number of symbol mismatches, insertions, and deletions) occurrences of a pattern string in a text string.  Preprocessing, e.g., building an index, may or may not be allowed.
</p>
<h2>Automatic basis function construction</h2>
<h2>Backward induction</h2>
<h2>Bellman equation</h2>
<h2>Bellman–Ford algorithm</h2>
<h2>Bitonic tour</h2>
<h2>Curse of dimensionality</h2>
<h2>Damerau–Levenshtein distance</h2>
<p>DamerauLevenshteinDistance[u, v]gives the Damerau-Levenshtein distance between strings or vectors u and v.Damerau-Levenshtein distance between two strings:Damerau-Levenshtein distance between two vectors:</p>
<h2>Differential dynamic programming</h2>
<h2>Dynamic time warping</h2>
<h2>Earley parser</h2>
<h2>Floyd–Warshall algorithm</h2>
<h2>Forward–backward algorithm</h2>
<h2>Hamilton–Jacobi–Bellman equation</h2>
<h2>Hirschberg's algorithm</h2>
<h2>Hunt–McIlroy algorithm</h2>
<h2>Knapsack problem</h2>
<p>To see full output on this page you need to enable JavaScript in your browser. More info »Zoom in to see an enlarged view of any output.AdvertisementBring Wolfram|Alpha output to life with CDF interactivity.</p>
<p>

Given items of different values and volumes, find the most valuable set of items that fit in a knapsack of fixed volume.
</p>
<h2>Levenshtein distance</h2>
<p>

(1) The smallest number of insertions, deletions, and substitutions required to change one string or tree into another.  (2) A Θ(m × n) algorithm to compute the distance between strings, where m and n are the lengths of the strings.
</p>
<p>DamerauLevenshteinDistance[u, v]gives the Damerau-Levenshtein distance between strings or vectors u and v.Damerau-Levenshtein distance between two strings:Damerau-Levenshtein distance between two vectors:</p>
<h2>Longest alternating subsequence</h2>
<h2>Longest common subsequence problem</h2>
<p>

The problem of finding a maximum length (or maximum weight) subsequence of two or more strings.
</p>
<h2>Longest common substring problem</h2>
<p>

The problem of finding a maximum length (or maximum weight) subsequence of two or more strings.
</p>
<h2>Longest increasing subsequence</h2>
<p>
The longest increasing scattered subsequence is the longest subsequence of increasing terms, where intervening nonincreasing terms may be dropped. Finding the largest
 scattered subsequence is a much harder problem. The longest increasing scattered
 subsequence of a partition can be found using LongestIncreasingSubsequence[p]
 in the Mathematica
 package Combinatorica` . For example, the longest increasing scattered subsequence
 of the permutation 
 is , whereas
 the longest contiguous subsequence is .
</p>
<h2>Markov decision process</h2>
<h2>Matrix chain multiplication</h2>
<p>

Given a sequence of matrices such that any matrix may be multiplied by the previous matrix, find the best association such that the result is obtained with the minimum  number of arithmetic operations.  One may use dynamic programming to find the best association.
</p>
<h2>Maximum subarray problem</h2>
<h2>Optimal stopping</h2>
<h2>Optimal substructure</h2>
<h2>Overlapping subproblems</h2>
<h2>Partially observable Markov decision process</h2>
<h2>Recursive economics</h2>
<h2>Shortest common supersequence</h2>
<p>

Find the shortest string that contains two or more strings as subsequences.
</p>
<h2>Smith–Waterman algorithm</h2>
<p>

A means of searching protein databases to find those with the best alignment.
</p>
<h2>Subset sum problem</h2>
<p>
There are two problems commonly known as the subset sum problem.

The first ("given sum problem") is the problem of finding what subset of a list of integers has a given sum, which is an integer
 relation problem where the relation coefficients  are 0 or 1.
</p>
<h2>Viscosity solution</h2>
<h2>Viterbi algorithm</h2>
<h2>Word wrap</h2>
<h2>Cycle sort</h2>
<p>
A permutation cycle is a subset of a permutation whose elements trade places with one another. Permutations cycles are called "orbits"
 by Comtet (1974, p. 256). For example, in the permutation
 group , (143) is a 3-cycle and (2)
 is a 1-cycle. Here, the notation (143) means that starting from the original ordering
 , the first element is replaced by the fourth,
 the fourth by the third, and the third by the first, i.e., .
</p>
<h2>Merge sort</h2>
<p>

A sort algorithm that splits the items to be sorted into two groups, recursively sorts each group, and  merges them into a final, sorted sequence.   Run time is Θ(n log n).
</p>
<p>
A merge sort (or collation sort) is the combination of two or more ordered lists into a single ordered list (Knuth 1998, p. 158). Merge sorting was one of the first methods proposed for computer sorting, and was first proposed by John von Neumann in 1945 (Knuth 1998, p. 159). Variants include two-way, natural two-way, straight two-way, and list merge sorts.
</p>
<h2>Bucket sort</h2>
<p>

A distribution sort where input elements are initially  distributed to several buckets based on an interpolation of the element's key. Each bucket is sorted if necessary, and the buckets' contents are concatenated.
</p>
<h2>Hungarian algorithm</h2>
<p>
The Hungarian algorithm finds a maximum independent edge set on a graph. The algorithm starts with any matching  and constructs a tree via a breadth-first
 search to find an augmenting path, namely a path
  that starts and finishes at unmatched
 vertices whose first and last edges are not in  and whose edges
 alternate being outside and inside . If the search
 succeeds, the symmetric difference of  and the edges in  yields a matching
 with one more edge than . That edge is
 added, and then another search is performed for a new augmenting
 path. If the search is unsuccessful, the algorithm terminates and  must be the largest-size
 matching.
</p>
<p>

Solve the assignment problem in polynomial time by marking and unmarking entries and covering and uncovering rows and columns.
</p>
<h2>Topological sorting</h2>
<p>
A topological sort is a permutation  of the vertices
 of a graph such that an edge  implies that
  appears before  in  (Skiena 1990, p. 208).
 Only directed acyclic graphs can be topologically
 sorted. The topological sort of a graph can be computed using TopologicalSort[g]
 in the Mathematica
 package Combinatorica` .
</p>
<h2>Edmonds's algorithm</h2>
<p>
The blossom algorithm (Edmonds 1965) finds a maximum independent edge set in a (possibly weighted) graph. While a maximum
 independent edge set can be found fairly easily for a bipartite
 graph using the Hungarian maximum
 matching algorithm, the general case is more difficult. Edmonds's blossom algorithm
 starts with a maximal independent edge
 set, which it tries to extend to a maximum
 independent edge set using the property that a matching
 is maximum iff the matching does not admit an augmenting
 path.
</p>
<h2>Minimum spanning tree</h2>
<p>
The minimum spanning tree of a weighted graph is a set of  edges of minimum total weight which
 form a spanning tree of the graph. When a graph
 is unweighted, any spanning tree is a minimum spanning
 tree.
</p>
<p>

A minimum-weight tree in a weighted graph which contains all of the graph's vertices.
</p>
<h2>Boruvka's algorithm</h2>
<p>

Compute a minimum spanning tree.
</p>
<p>The #1 tool for creating Demonstrations and anything technical.Explore anything with the first computational knowledge engine.Explore thousands of free applications across science, mathematics, engineering, technology, business, art, finance, social sciences, and more.</p>
<h2>Shortest path problem</h2>
<p>
The problem of finding the shortest path (a.k.a. graph geodesic) connecting two specific vertices  of a directed
 or undirected graph. The length of the graph geodesic
 between these points  is called
 the graph distance between  and . Common algorithms
 for solving the shortest path problem include the Bellman-Ford
 algorithm and Dijkstra's algorithm.
</p>
<p>

The problem of finding the shortest path in a graph  from one vertex to another.  "Shortest" may be least number of edges, least total weight, etc.
</p>
<h2>Bellman–Ford algorithm</h2>
<p>

An efficient algorithm to solve the single-source shortest-path problem. Weights may be negative.   The algorithm initializes the distance to the source vertex to 0 and all other vertices to ∞.  It then does V-1 passes (V is the number of vertices) over all edges relaxing, or updating, the distance to the destination of each edge. Finally it checks each edge again to detect negative weight cycles, in which case it returns false.  The time complexity is O(VE), where E is the number of edges.
</p>
<p>
An algorithm for solving the shortest path problem, i.e., finding a graph geodesic between two given
 vertices. Other algorithms that can be used for this purpose include Dijkstra's
 algorithm and reaching algorithm. The algorithm
 is implemented as BellmanFord[g,
 v] in the Mathematica
 package Combinatorica` .
</p>
<p>

An efficient algorithm to solve the single-source shortest-path problem. Weights may be negative.   The algorithm initializes the distance to the source vertex to 0 and all other vertices to ∞.  It then does V-1 passes (V is the number of vertices) over all edges relaxing, or updating, the distance to the destination of each edge. Finally it checks each edge again to detect negative weight cycles, in which case it returns false.  The time complexity is O(VE), where E is the number of edges.
</p>
<p>
An algorithm for solving the shortest path problem, i.e., finding a graph geodesic between two given
 vertices. Other algorithms that can be used for this purpose include Dijkstra's
 algorithm and reaching algorithm. The algorithm
 is implemented as BellmanFord[g,
 v] in the Mathematica
 package Combinatorica` .
</p>
<h2>Traveling salesman problem</h2>
<p>
A problem in graph theory requiring the most efficient (i.e., least total distance) Hamiltonian cycle
 a salesman can take through each of  cities. No general
 method of solution is known, and the problem is NP-hard.
 Solution to the traveling salesman problem is implemented as TravelingSalesman[g]
 in the Mathematica
 package Combinatorica`  and FindShortestTour[v1,
 v2, ...].
</p>
<p>

Find a path of minimum Euclidean distance between points in a plane which includes each point exactly once and returns to its starting point.
</p>
<h2>Backtracking</h2>
<p>

Find a solution by trying one of several choices.  If the choice proves incorrect, computation backtracks or restarts at the point of choice and tries another choice.  It is often convenient to maintain choice points and alternate choices using recursion.
</p>
<p>
A method of solving combinatorial problems by means of an algorithm which is allowed to run forward until a dead end is reached, at which point previous steps are retraced
 and the algorithm is allowed to run forward again. Backtracking can greatly reduce
 the amount of work in an exhaustive search. Backtracking is implemented as Backtrack[s,
 partialQ, solutionQ] in the Mathematica
 package Combinatorica` .
</p>
<h2>Priority queue</h2>
<p>

An abstract data type to efficiently support finding the item with the highest priority across a series of operations.  The basic operations are: insert, find-minimum (or maximum), and delete-minimum (or maximum).  Some implementations also efficiently support join two priority queues (meld), delete an arbitrary item, and increase the priority of a item (decrease-key).
</p>
<h2>Breadth-first search</h2>
<p>

Any search algorithm that considers neighbors of a vertex, that is, outgoing edges of the vertex's predecessor in the search, before any outgoing edges of the vertex.  Extremes are searched last.  This is typically implemented with a queue.
</p>
<h2>Depth-first search</h2>
<p>

(1) Any search algorithm that considers outgoing edges (children) of a vertex before any of the vertex's siblings, that is, outgoing  edges of the vertex's predecessor in the search.  Extremes are searched first.  This is easily implemented with recursion.  (2) An algorithm that marks all vertices in a directed graph in the order they are discovered and finished, partitioning the graph into a forest.
</p>
<h2>Strongly connected components</h2>
<p>
A strongly connected component is maximal subgraph of a directed graph such that for every pair of vertices
 ,  in the subgraph,
 there is a directed path from  to  and a directed
 path from  to . Tarjan (1972)
 has devised an  algorithm for
 determining strongly connected components, which is implemented in Mathematica
 as StronglyConnectedComponents[g]
 in the Mathematica
 package Combinatorica`  (Skiena 1990, p. 172) and StrongComponents[g]
 in the Mathematica
 package GraphUtilities` .
</p>
<p>

A strongly connected subgraph, S, of a directed graph, D, such that no vertex of D can be added to S and it still be strongly connected.  Informally, a maximal subgraph in which every vertex is reachable from every other vertex.
</p>
<h2>Tarjan's strongly connected components algorithm</h2>
<p>
A strongly connected component is maximal subgraph of a directed graph such that for every pair of vertices
 ,  in the subgraph,
 there is a directed path from  to  and a directed
 path from  to . Tarjan (1972)
 has devised an  algorithm for
 determining strongly connected components, which is implemented in Mathematica
 as StronglyConnectedComponents[g]
 in the Mathematica
 package Combinatorica`  (Skiena 1990, p. 172) and StrongComponents[g]
 in the Mathematica
 package GraphUtilities` .
</p>
<p>

A strongly connected subgraph, S, of a directed graph, D, such that no vertex of D can be added to S and it still be strongly connected.  Informally, a maximal subgraph in which every vertex is reachable from every other vertex.
</p>
<h2>Floyd–Warshall algorithm</h2>
<p>
The Floyd-Warshall algorithm, also variously known as Floyd's algorithm, the Roy-Floyd algorithm, the Roy-Warshall algorithm, or the WFI algorithm, is an algorithm for
 efficiently and simultaneously finding the shortest paths (i.e., graph
 geodesics) between every pair of vertices in a weighted and potentially directed
 graph.
</p>
<p>

An algorithm to solve the all pairs shortest path problem in a weighted, directed graph by multiplying an adjacency-matrix representation of the graph multiple times.  The edges may have negative weights, but no negative weight cycles. The time complexity is Θ (V³).
</p>
<p>
The Floyd-Warshall algorithm, also variously known as Floyd's algorithm, the Roy-Floyd algorithm, the Roy-Warshall algorithm, or the WFI algorithm, is an algorithm for
 efficiently and simultaneously finding the shortest paths (i.e., graph
 geodesics) between every pair of vertices in a weighted and potentially directed
 graph.
</p>
<p>

An algorithm to solve the all pairs shortest path problem in a weighted, directed graph by multiplying an adjacency-matrix representation of the graph multiple times.  The edges may have negative weights, but no negative weight cycles. The time complexity is Θ (V³).
</p>
<h2>Needleman–Wunsch algorithm</h2>
<p>Embed Interactive Demonstration New!Files require Wolfram CDF Player or Mathematica.</p>
<h2>String searching algorithm</h2>
<p>

The problem of finding occurrence(s) of a pattern string  within another string or body of text.  There are many different algorithms for efficient searching.
</p>
<h2>Aho–Corasick string matching algorithm</h2>
<p>

A multiple string matching algorithm that constructs a  finite state machine from a pattern (list of keywords), then uses the machine to locate all occurrences of the keywords in a body of text.
</p>
<h2>Apostolico–Giancarlo algorithm</h2>
<p>

(no definition here, yet, but
you can help.)
</p>
<h2>Bitap algorithm</h2>
<h2>Boyer–Moore string search algorithm</h2>
<h2>Levenshtein automaton</h2>
<h2>Raita Algorithm</h2>
<h2>Lexicographically minimal string rotation</h2>
<h2>Longest palindromic substring</h2>
<h2>Longest repeated substring problem</h2>
<h2>String-to-string correction problem</h2>
<h2>Hellinger distance</h2>
<h2>Lee distance</h2>
<h2>Overlap coefficient</h2>
<h2>Sørensen–Dice coefficient</h2>
<h2>String kernel</h2>
<h2>Tversky index</h2>
<h2>Wagner–Fischer algorithm</h2>
<h2>Probalign</h2>
<h2>Cycle detection</h2>
<h2>Floyd's cycle-finding algorithm</h2>
<h2>Pseudorandom number generator</h2>
<p>

A deterministic algorithm to generate a sequence of numbers with little or no discernible pattern in the numbers, except for broad statistical properties.
</p>
<p>
A slightly archaic term for a computer-generated random number. The prefix pseudo- is used to distinguish this type of number from a
 "truly" random number generated by a random
 physical process such as radioactive decay.
</p>
<h2>Lagged Fibonacci generator</h2>
<h2>Flow network</h2>
<p>

A weighted, directed graph with two specially marked nodes, the source s and the sink t, and a capacity function that maps edges to positive real numbers, u: E |â†’ R+.
</p>
<h2>General Problem Solver</h2>
<h2>Phonetic algorithm</h2>
<h2>Daitch–Mokotoff Soundex</h2>
<h2>Double Metaphone algorithm</h2>
<p>

An algorithm to code English words (and foreign words often heard in the United States) phonetically by reducing them to a combination of 12 consonant sounds.   It returns two codes if a word has two plausible pronunciations, such as a foreign word. This reduces matching problems from wrong spelling.
</p>
<h2>Match Rating Approach</h2>
<h2>Metaphone algorithm</h2>
<p>

An algorithm to code English words (and foreign words often heard in the United States) phonetically by reducing them to a combination of 12 consonant sounds.   It returns two codes if a word has two plausible pronunciations, such as a foreign word. This reduces matching problems from wrong spelling.
</p>
<h2>New York State Identification and Intelligence System</h2>
<p>

Convert a name to a phonetic coding of up to six characters.
</p>
<h2>Soundex algorithm</h2>
<h2>String metrics</h2>
<h2>Dice's coefficient</h2>
<h2>Trigram search</h2>
<h2>Selection algorithm</h2>
<p>Automatic algorithm selection (AAS) is the underlying technology that			enables Mathematica to select and apply the best algorithm(s)			for a given task.System-wide implementation of AAS is unique to Mathematica and a key			distinguishing feature: other technical computing systems make the user			specify a single algorithm (not just the task) by hand, often from a confusing			array of possibilities. Get the selection wrong and your computation			could fail or, worse still, produce an inaccurate answer.</p>
<h2>Ternary search</h2>
<p>

A 3-way tree where every node's left  subtree has keys less than the node's key, every middle subtree has keys equal to the node's key, and every right subtree has keys greater than the node's key.  If the key is a multikey (string, array, list, etc.), the middle subtree organizes by the next subkey (character, array or list item, etc.)
</p>
<h2>Sorted list</h2>
<h2>Fibonacci search technique</h2>
<h2>Uniform binary search</h2>
<h2>Fisher–Yates shuffle</h2>
<h2>Schensted algorithm</h2>
<h2>Steinhaus–Johnson–Trotter algorithm</h2>
<h2>Kadane's algorithm</h2>
<h2>Substring search</h2>
<h2>Rabin–Karp string search algorithm</h2>
<h2>Ukkonen's algorithm</h2>
<p>

A compact representation of a trie corresponding to the suffixes of a given string where all nodes with one child are merged with their parents.
</p>
<h2>AA tree</h2>
<h2>Randomized binary search tree</h2>
<p>

A binary search tree in which nodes have a randomly assigned priority.  Updates keep priorities in heap order instead of keeping balance information and doing rebalance operations.
</p>
<h2>Rope (computer science)</h2>
<h2>Self-balancing binary search tree</h2>
<h2>T-tree</h2>
<h2>Tango tree</h2>
<h2>Threaded binary tree</h2>
<h2>Top tree</h2>
<h2>B+ tree</h2>
<p>

A balanced search tree in which every node has between  m/2 and m children, where m>1 is a fixed integer.  m is the order.  The root may have as few as 2 children. This is a good structure if much of the tree is in slow memory (disk), since the height, and hence the number of accesses, can be kept small, say one or two, by picking a large m.
</p>
<h2>B*-tree</h2>
<p>

A B-tree in which nodes are kept 2/3 full by redistributing keys to fill two child nodes, then splitting them into three nodes.
</p>
<h2>Dancing tree</h2>
<h2>2-3-4 tree</h2>
<p>

A B-tree of order 4, that is, internal nodes have two, three, or four children.
</p>
<h2>Queap</h2>
<h2>Fusion tree</h2>
<h2>Bx-tree Moving Object Index</h2>
<h2>Heap (data structure)</h2>
<h2>AF-heap</h2>
<h2>Soft heap</h2>
<h2>Pairing heap</h2>
<h2>Beap</h2>
<h2>Skew heap</h2>
<h2>Ternary heap</h2>
<h2>D-ary heap</h2>
<h2>Brodal queue</h2>
<h2>Tree</h2>
<p>
A tree is a mathematical structure that can be viewed as either a graph or as a data structure. The two views are equivalent,
 since a tree data structure contains not only a
 set of elements, but also connections between elements, giving a tree graph.
</p>
<h2>Suffix tree</h2>
<p>

A compact representation of a trie corresponding to the suffixes of a given string where all nodes with one child are merged with their parents.
</p>
<h2>Suffix array</h2>
<p>

An array of all starting positions of suffixes of a string arranged in lexicographical order. This allows a binary search or fast substring search.
</p>
<h2>Compressed suffix array</h2>
<h2>Kruskal's algorithm</h2>
<p>
An algorithm for finding a graph's spanning tree of minimum length. It sorts the edges
 of a graph in order of increasing cost and then repeatedly adds edges that bridge
 separate components until the graph is fully connected (Pemmaraju and Skiena 2003,
 p. 336). By negating the weights for each edge, the algorithm can also be used
 to find a maximum spanning tree.
</p>
<p>

An algorithm for computing a minimum spanning tree.  It maintains a set of partial minimum spanning trees, and repeatedly adds the shortest edge in the graph whose vertices are in different partial minimum spanning trees.
</p>
<h2>Alpha-beta pruning</h2>
<h2>Branch and bound</h2>
<h2>Bruss algorithm</h2>
<h2>Chain matrix multiplication</h2>
<h2>Combinatorial optimization</h2>
<h2>Greedy randomized adaptive search procedure</h2>
<h2>Hungarian method</h2>
<h2>Min conflicts algorithm</h2>
<h2>Chaff algorithm</h2>
<h2>Davisâ€“Putnam algorithm</h2>
<h2>Ellipsoid method</h2>
<h2>Fitness proportionate selection</h2>
<h2>Stochastic universal sampling</h2>
<h2>Tournament selection</h2>
<h2>Harmony search</h2>
<h2>Red-black tree</h2>
<p>
An extended rooted binary
tree satisfying the following conditions: 

1. Every node has two children, each colored either red
or black. 

2. Every tree leaf node is colored black. 

3. Every red node has both of its children colored black.
</p>
<p>

A nearly-balanced tree that uses an extra bit per  node to maintain balance.  No leaf is more than twice as far from the root as any other.
</p>
<h2>Scapegoat tree</h2>
<p>

A binary search tree that needs no balance information. Search time is logarithmic, and the amortized cost of update is logarithmic.
</p>
<h2>Weight-balanced tree</h2>
<p>

A binary tree where the balance of every subtree,  Ï(T'), is bounded by Î± â‰¤ Ï(T') â‰¤ 1-Î±.
</p>
<p>
A tree to whose nodes and/or edges labels (usually number)
are assigned.

The word "weight" also has a more specific meaning when applied to trees, namely the weight of a tree at a point  is the maximum
 number of edges in any branch at  (Harary 1994, p.Â 35),
 as illustrated above. A point having minimal weight for the tree is called a centroid
 point, and the tree centroid is the set of all
 centroid points.
</p>
<h2>Binary tree</h2>
<p>

A tree with at most two children for each  node.
</p>
<h2>Cartesian tree</h2>
<p>

A binary search tree in which nodes have a randomly assigned priority.  Updates keep priorities in heap order instead of keeping balance information and doing rebalance operations.
</p>
<p>Embed Interactive Demonstration New!Files require Wolfram CDF Player or Mathematica.</p>
<h2>B-tree</h2>
<p>

A balanced search tree in which every node has between  m/2 and m children, where m>1 is a fixed integer.  m is the order.  The root may have as few as 2 children. This is a good structure if much of the tree is in slow memory (disk), since the height, and hence the number of accesses, can be kept small, say one or two, by picking a large m.
</p>
<p>
-trees were introduced by Bayer (1972) and McCreight. They are
 a special -ary balanced tree used in databases because their
 structure allows records to be inserted, deleted, and retrieved with guaranteed worst-case
 performance. An -node -tree has height
 , where lg is the logarithm to base 2. The AppleÂ® MacintoshÂ® (Apple,
 Inc., Cupertino, CA) HFS filing system uses -trees to store
 disk directories (Benedict 1995). A -tree satisfies
 the following properties: 
</p>
<h2>Fibonacci heap</h2>
<p>

A heap made of a forest of trees.  The amortized cost of the operations create, insert a value, decrease a value, find minimum, and merge or join (meld) two heaps, is a constant Î˜(1).  The delete operation takes O(log n).
</p>
<h2>2-3 heap</h2>
<p>
A sequence  forms
 a (binary) heap if it satisfies  for , where
  is the floor function,
 which is equivalent to  and 
 for . The first member must therefore
 be the smallest. A heap can be viewed as a labeled binary
 tree in which the label of the th node is smaller
 than the labels of any of its descendents (Skiena 1990, p.Â 35). Heaps support
 arbitrary insertion and seeking/deletion of the minimum value in  times per
 update (Skiena 1990, p.Â 38).
</p>
<h2>Leftist tree</h2>
<p>

A priority queue implemented with a variant of a binary tree.  Every node has a count which is the distance to the nearest leaf.  In addition to the heap property, leftist trees are kept so the right descendant of each node has the shorter distance to a leaf.
</p>
<h2>Radix tree</h2>
<p>

A compact representation of a trie in which any node that is an only child is merged with its  parent.
</p>
<h2>Boyer–Moore–Horspool algorithm</h2>
<p>

A string matching algorithm that compares characters from the end of the pattern to its beginning.  When characters don't match, searching jumps to the next matching position in the pattern.
</p>
<h2>Knuth–Morris–Pratt algorithm</h2>
<p>

A string matching algorithm that turns the search string into a finite state machine, then runs the machine with the string to be searched as the input string.  Execution time is O(m+n), where m is the length of the search string, and n is the length of the string to be searched.
</p>
<h2>Zhu–Takaoka string matching algorithm</h2>
<p>

A string matching algorithm that is a variant of the Boyer-Moore algorithm.  It uses two consecutive text characters to compute the bad character shift.  It is faster when the alphabet or pattern is small, but the skip table grows quickly, slowing the pre-processing phase.
</p>
<h2>Edit distance</h2>
<p>EditDistance[u, v]gives the edit or Levenshtein distance between strings or vectors u and v.Edit distance between two strings:Edit distance between two vectors:DamerauLevenshteinDistance  HammingDistance  SmithWatermanSimilarity  StringCount  JaccardDissimilarity</p>
<h2>Euclidean distance</h2>
<p>
The distance between two points is the length of the path connecting them. In the plane, the distance between points  and  is given by the Pythagorean
 theorem,

In Euclidean three-space, the distance between points  and
  is
</p>
<p>

The straight line distance between two points.  In a plane with p1 at (x1, y1) and p2 at (x2, y2), it is  √((x1 - x2)² + (y1 - y2)²).
</p>
<h2>Sequence alignment</h2>
<p>Mathematica includes state-of-the-art algorithms for sequence alignment and comparison, capable of handling strings and lists containing very large numbers of elements.SequenceAlignment — find alignments between sequences, allowing insertion and deletion</p>
<h2>Commentz-Walter algorithm</h2>
<p>

A multiple string matching algorithm that compares from the end of the pattern, like Boyer-Moore, using a finite state machine, like Aho-Corasick.
</p>
<h2>Rabin–Karp algorithm</h2>
<p>

A string matching algorithm that compares string's  hash values, rather than the strings themselves.  For efficiency, the hash value of the next position in the text is easily computed from the hash value of the current position.
</p>
<h2>Hamming distance</h2>
<p>HammingDistance[u, v] gives the Hamming distance between strings or vectors u and v.Hamming distance between two strings:Hamming distance between two vectors:EditDistance  JaccardDissimilarity  BitXor  DigitCount</p>
<p>

The number of bits which differ between two binary strings. More formally, the distance between two strings A and B is ∑ | Ai - Bi |.
</p>
<h2>Inversion (discrete mathematics)</h2>
<p>Permutations are among the most basic elements of discrete mathematics. They are used to represent discrete groups of transformations, and in particular play a key role in group theory, the mathematical study of symmetry. Permutations and groups are important in many aspects of life. We all live on a giant sphere (the Earth) whose rotations are described by the group SO(3) (the special orthogonal group in 3 dimensions). On the micro-scale, the Hungarian-American physicist Eugene Wigner (November 17, 1902–January 1, 1995), who received a share of the Nobel Prize in Physics in 1963, discovered the “electron permutation group”, one of many applications of permutation groups to quantum mechanics.</p>
<h2>Jaccard index</h2>
<p>JaccardDissimilarity[u, v]gives the Jaccard dissimilarity between Boolean vectors u and v.Jaccard dissimilarity between two Boolean vectors:The elements can also be True and False:MatchingDissimilarity  DiceDissimilarity  SokalSneathDissimilarity  RogersTanimotoDissimilarity  RussellRaoDissimilarity  YuleDissimilarity</p>
<h2>Jaro–Winkler distance</h2>
<p>

A measure of similarity between two strings.  The Jaro measure is the weighted sum of percentage of matched characters from each file and transposed characters.  Winkler increased this measure for matching initial characters, then rescaled it by a piecewise function, whose intervals and weights depend on the type of string (first name, last name, street, etc.).
</p>
<h2>Stable marriage problem</h2>
<p>
Given a set of  men and  women, marry them
 off in pairs after each man has ranked the women in order of preference from 1 to
 ,  and
 each women has done likewise, . If
 the resulting set of marriages contains no pairs of the
 form ,  such that
  prefers  to  and  prefers  to , the marriage
 is said to be stable. Gale and Shapley (1962) showed that a stable marriage exists
 for any choice of rankings (Skiena 1990, p.Â 245). In the United States, the
 algorithm of Gale and Shapley (1962) is used to match hospitals to medical interns
 (Skiena 1990, p.Â 245).
</p>
<h2>Blum Blum Shub</h2>
<p>The ability to generate pseudorandom numbers is important for simulating events, estimating probabilities and other quantities, making randomized assignments or selections, and numerically testing symbolic results. Such applications may require uniformly distributed numbers, nonuniformly distributed numbers, elements sampled with replacement, or elements sampled without replacement. </p>
<h2>Linear congruential generator</h2>
<p>
A method for generating random (pseudorandom) numbers using the linear recurrence relation

where  and  must assume certain
 fixed values,  is some chosen modulus, and  is an initial
 number known as the seed.
</p>
<p>

A class of algorithms that are pseudo-random number generators.  The next number is generated from the current one by  rn+1 = (A Ã— rn + B) mod  M, where A and M are relatively prime numbers.
</p>
<h2>Mersenne twister</h2>
<p>Embed Interactive Demonstration New!Files require Wolfram CDF Player or Mathematica.</p>
<h2>Linear search</h2>
<p>

Search an array or list by checking items one at a time.
</p>
<h2>Binary search algorithm</h2>
<p>

Search a sorted array by repeatedly dividing the search interval in half.  Begin with an interval covering the whole array. If the value of the search key is less than the item in the middle of the interval, narrow the interval to the lower half.  Otherwise narrow it to the upper half.  Repeatedly check until the value is found or the interval is empty.
</p>
<p>
A searching algorithm which works on a sorted table by testing the middle of an interval, eliminating the half of the table in which
 the key cannot lie, and then repeating the procedure iteratively.
</p>
<h2>Jump search</h2>
<p>

Search a sorted array by checking every jth item until the right area is found, then doing a linear search.  The optimum for n items is when j=âˆš n.
</p>
<h2>Interpolation search</h2>
<p>

Search a sorted array by estimating the next position to check based on a linear interpolation of the search key and the values at the ends of the search interval.
</p>
<p>Embed Interactive Demonstration New!Files require Wolfram CDF Player or Mathematica.</p>
<h2>Longest increasing subsequence problem</h2>
<p>
The longest increasing scattered subsequence is the longest subsequence of increasing terms, where intervening nonincreasing terms may be dropped. Finding the largest
 scattered subsequence is a much harder problem. The longest increasing scattered
 subsequence of a partition can be found using LongestIncreasingSubsequence[p]
 in the Mathematica
 package Combinatorica` . For example, the longest increasing scattered subsequence
 of the permutation 
 is , whereas
 the longest contiguous subsequence is .
</p>
<h2>FM-index</h2>
<h2>Generalised suffix tree</h2>
<h2>Judy array</h2>
<h2>Ctree</h2>
<h2>K-ary tree</h2>
<h2>Andâ€“or tree</h2>
<h2>(a,b)-tree</h2>
<h2>Link/cut tree</h2>
<h2>SPQR-tree</h2>
<h2>Disjoint-set data structure</h2>
<h2>Enfilade (Xanadu)</h2>
<h2>Fenwick tree</h2>
<h2>Van Emde Boas tree</h2>
<h2>Segment tree</h2>
<h2>Interval tree</h2>
<h2>Range tree</h2>
<h2>Bin (computational geometry)</h2>
<h2>Kd-tree</h2>
<h2>Min/max kd-tree</h2>
<h2>Adaptive k-d tree</h2>
<h2>Quadtree</h2>
<h2>Octree</h2>
<h2>Linear octree</h2>
<h2>UB-tree</h2>
<h2>R-tree</h2>
<h2>R+ tree</h2>
<h2>R* tree</h2>
<h2>Hilbert R-tree</h2>
<h2>X-tree</h2>
<h2>Metric tree</h2>
<h2>Cover tree</h2>
<h2>M-tree</h2>
<h2>VP-tree</h2>
<h2>Bounding interval hierarchy</h2>
<h2>BSP tree</h2>
<h2>Rapidly exploring random tree</h2>
<h2>Abstract syntax tree</h2>
<h2>Parse tree</h2>
<h2>Decision tree</h2>
<h2>Alternating decision tree</h2>
<h2>Minmax</h2>
<h2>Expectiminimax tree</h2>
<h2>Algorithms for calculating variance</h2>
<h2>Approximate counting algorithm</h2>
<h2>Bayesian statistics</h2>
<h2>Nested sampling algorithm</h2>
<h2>UPGMA</h2>
<h2>DBSCAN</h2>
<h2>Linde–Buzo–Gray algorithm</h2>
<h2>OPTICS algorithm</h2>
<h2>RANSAC</h2>
<h2>AdaBoost</h2>
<h2>LPBoost</h2>
<h2>Perceptron</h2>
<h2>Reinforcement Learning</h2>
<h2>SARSA</h2>
<h2>Temporal difference learning</h2>
<h2>Support Vector Machines</h2>
<h2>Structured SVM</h2>
